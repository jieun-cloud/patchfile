diff --git FIRMWARE/firm_buffer_manager.c FIRMWARE/firm_buffer_manager.c
index 6e27fe8..ca5efe1 100644
--- FIRMWARE/firm_buffer_manager.c
+++ FIRMWARE/firm_buffer_manager.c
@@ -696,6 +696,58 @@ event_queue_entry* CREATE_NEW_EVENT(int io_type, uint64_t slba, uint32_t nlb, vo
 {
 	static uint64_t seq_nb = 0;
 
+	// Allocate new event entry 
+	event_queue_entry* new_eq_entry = calloc(1, sizeof(event_queue_entry));
+	if(new_eq_entry == NULL){
+		printf("[%s] Allocation new event fail.\n", __FUNCTION__);
+		return NULL;
+	}
+
+	// Allocate sequence number for this event 
+	new_eq_entry->seq_nb = seq_nb;
+	seq_nb++;
+
+	// Initialize new event 
+	new_eq_entry->io_type = io_type;
+	new_eq_entry->valid = VALID;
+	new_eq_entry->sector_nb = slba;
+	new_eq_entry->length = nlb;
+	new_eq_entry->cb = cb;
+	new_eq_entry->opaque = opaque;
+	new_eq_entry->buf = NULL;
+	new_eq_entry->n_child = 0;
+	new_eq_entry->n_completed = 0;
+	new_eq_entry->n_trimmed = 0;
+	new_eq_entry->e_state = WAIT_CHILD;
+	
+	/* Jieun add 20.11.24 */
+	new_eq_entry->stream_id = 0;
+	new_eq_entry->epoch_id = 0;
+	new_eq_entry->barrier_flag = 0;
+
+	pthread_mutex_init(&new_eq_entry->lock, NULL);
+	new_eq_entry->flush = false;
+
+	new_eq_entry->t_start = get_usec();
+	new_eq_entry->n_pages = 0;
+
+	new_eq_entry->prev = NULL;
+	new_eq_entry->next = NULL;
+
+	return new_eq_entry;
+}
+
+
+
+/* Jieun add 20.11.24 
+   This function is for barrier-enabled NVMe FTL 
+   Every event queue entry has stream id, epoch id and barrier flag. 
+   If the request is not a order-preserving write request, the stream id, epoch id, and barrier flag will be zero. 
+   */
+event_queue_entry* CREATE_NEW_EVENT_BARRIER(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb, int stream_id, uint32_t epoch_id, int barrier_flag)
+{
+	static uint64_t seq_nb = 0;
+
 	/* Allocate new event entry */
 	event_queue_entry* new_eq_entry = calloc(1, sizeof(event_queue_entry));
 	if(new_eq_entry == NULL){
@@ -720,6 +772,12 @@ event_queue_entry* CREATE_NEW_EVENT(int io_type, uint64_t slba, uint32_t nlb, vo
 	new_eq_entry->n_trimmed = 0;
 	new_eq_entry->e_state = WAIT_CHILD;
 
+	/* Jieun add 20.11.24 */
+
+	new_eq_entry->stream_id = stream_id;
+	new_eq_entry->epoch_id = epoch_id;
+	new_eq_entry->barrier_flag = barrier_flag;
+	
 	pthread_mutex_init(&new_eq_entry->lock, NULL);
 	new_eq_entry->flush = false;
 
@@ -731,7 +789,6 @@ event_queue_entry* CREATE_NEW_EVENT(int io_type, uint64_t slba, uint32_t nlb, vo
 
 	return new_eq_entry;
 }
-
 /* This function should be called after eq_entry->lock is already held. */
 void UPDATE_EVENT_STATE(event_queue_entry* eq_entry, enum event_state state)
 {
diff --git FIRMWARE/firm_buffer_manager.h FIRMWARE/firm_buffer_manager.h
index abb27bc..25575f9 100644
--- FIRMWARE/firm_buffer_manager.h
+++ FIRMWARE/firm_buffer_manager.h
@@ -82,6 +82,11 @@ typedef struct event_queue_entry
 	/* Bandwidth */
 	int64_t t_start;
 	uint32_t n_pages;
+	
+	/* Jieun add 20.11.24 */
+	int stream_id;
+	uint32_t epoch_id;
+	int barrier_flag;
 
 	/* pointers for candidate queue */
 	struct event_queue_entry* prev;
@@ -105,6 +110,8 @@ event_queue_entry* DEQUEUE_IO(void);
 /* Manipulate event queue entries */
 event_queue_entry* CREATE_NEW_EVENT(int io_type, uint64_t slba, 
 			uint32_t nlb, void* opaque, CallbackFunc *cb);
+
+event_queue_entry* CREATE_NEW_EVENT_BARRIER(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb, int stream_id, uint32_t epoch_id, int barrier_flag); //Jieun add 20.11.24
 void UPDATE_EVENT_STATE(event_queue_entry* eq_entry, enum event_state state); 
 int GET_EVENT_STATE(event_queue_entry* eq_entry); 
 int GET_N_IO_PAGES(uint64_t sector_nb, uint32_t length);
diff --git FIRMWARE/ssd.c FIRMWARE/ssd.c
index 005d8d8..0997c26 100644
--- FIRMWARE/ssd.c
+++ FIRMWARE/ssd.c
@@ -51,9 +51,10 @@ event_queue_entry* SSD_NVME_READ(uint64_t slba, uint32_t nlb, NvmeRequest *req,
 		void(*cb)(void *opaque, int ret))
 {
 	return SSD_RW(READ, slba, nlb, req, cb);
-}
 
+}
 
+/*
 event_queue_entry* SSD_NVME_WRITE(uint64_t slba, uint32_t nlb, NvmeRequest *req,
 		void(*cb)(void *opaque, int ret))
 {
@@ -64,6 +65,20 @@ event_queue_entry* SSD_NVME_WRITE(uint64_t slba, uint32_t nlb, NvmeRequest *req,
 
 	return SSD_RW(WRITE, slba, nlb, req, cb);
 }
+*/
+
+/* Jieun add 20.11.24 */
+event_queue_entry* SSD_NVME_WRITE_BARRIER(uint64_t slba, uint32_t nlb, NvmeRequest *req,
+		void(*cb)(void *opaque, int ret), int stream_id, uint32_t epoch_id, int barrier_flag)
+{
+	if(nlb > N_WB_SECTORS){
+		printf("ERROR[%s] the size of the write event (%u) exceeds the write buffer (%d), please increase the write buffer size in the ssd configuration \n", __FUNCTION__, nlb, N_WB_SECTORS);
+		return NULL;
+	}
+
+//	return SSD_RW(WRITE, slba, nlb, req, cb);
+	return SSD_RW_BARRIER(WRITE, slba, nlb, req, cb, stream_id, epoch_id, barrier_flag); //Jieun add 20.11.24
+}
 
 
 event_queue_entry* SSD_NVME_FLUSH(uint64_t slba, uint32_t nlb, NvmeRequest *req,
@@ -100,6 +115,25 @@ event_queue_entry* SSD_RW(int io_type, uint64_t slba, uint32_t nlb, void* opaque
 	return new_eq_entry;
 }
 
+/* Jieun add 20.11.24 *
+   This function is for barrier-enabled NVMe FTL.
+   SSD_WRITE make a event queue entry which includes stream id, epoch id and barrier flag from Host. */
+event_queue_entry* SSD_RW_BARRIER(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb,  int stream_id, uint32_t epoch_id, int barrier_flag)
+{
+	event_queue_entry* new_eq_entry = NULL;
+
+	/* Create new I/O event */
+	new_eq_entry = CREATE_NEW_EVENT_BARRIER(io_type, slba, nlb, opaque, cb, stream_id, epoch_id, barrier_flag);
+
+	/* Insert new I/O event to the event queue*/
+	ENQUEUE_IO(new_eq_entry);
+
+	/* Wake up the firmware io buffer thread */
+	pthread_cond_signal(&eq_ready);
+
+	return new_eq_entry;
+}
+
 
 void SSD_DSM_DISCARD(NvmeRequest *req, uint32_t nr)
 {
diff --git FIRMWARE/ssd.h FIRMWARE/ssd.h
index ded0310..0285ca4 100644
--- FIRMWARE/ssd.h
+++ FIRMWARE/ssd.h
@@ -28,6 +28,12 @@ event_queue_entry* SSD_NVME_FLUSH(uint64_t slba, uint32_t nlb, NvmeRequest *req,
 
 event_queue_entry* SSD_RW(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb);
 
+/* Jieun add */
+event_queue_entry* SSD_RW_BARRIER(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb,  int stream_id, uint32_t epoch_id, int barrier_flag);
+event_queue_entry* SSD_NVME_WRITE_BARRIER(uint64_t slba, uint32_t nlb, NvmeRequest *req, void(*cb)(void *opaque, int ret), int stream_id, uint32_t epoch_id, int barrier_flag);
+
+
+
 /* TRIM command support */
 void SSD_DSM_DISCARD(NvmeRequest *req, uint32_t nr);
 int IS_SSD_TRIM_ENABLED(void);
diff --git FIRMWARE/vssim_core.c FIRMWARE/vssim_core.c
index 46d86f0..a4b0b93 100644
--- FIRMWARE/vssim_core.c
+++ FIRMWARE/vssim_core.c
@@ -309,6 +309,8 @@ void *FIRM_IO_BUF_THREAD_MAIN_LOOP(void *arg)
 
 		/* Get new IO event */
 		cur_entry = DEQUEUE_IO();
+		// Jieun add for debugging
+	//	printf("FIRM_IO_BUF_THREAD_MAIN_LOOP!!! stream_id: %d\t epoch id: %d\t barrier: %d\n", cur_entry->stream_id, cur_entry->epoch_id, cur_entry->barrier_flag);
 		
 		pthread_mutex_unlock(&eq_lock);
 
diff --git QEMU/hw/block/nvme.c QEMU/hw/block/nvme.c
index 7d08f62..d93f04c 100644
--- QEMU/hw/block/nvme.c
+++ QEMU/hw/block/nvme.c
@@ -248,6 +248,8 @@ static uint16_t nvme_flush(NvmeCtrl *n, NvmeNamespace *ns, NvmeCmd *cmd,
          BLOCK_ACCT_FLUSH);
 
 #ifdef VSSIM_NVME
+	/* Jieun 20.11.16 */
+	printf("\n CMD:FLUSH \n");
     event_queue_entry* vssim_event = NULL; 
     vssim_event = SSD_NVME_FLUSH(0, 0, req, nvme_rw_cb);
 
@@ -276,6 +278,12 @@ static uint16_t nvme_rw(NvmeCtrl *n, NvmeNamespace *ns, NvmeCmd *cmd,
     int is_write = rw->opcode == NVME_CMD_WRITE ? 1 : 0;
     enum BlockAcctType acct = is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ;
 
+	/* Jieun 20.11.16 */
+	int control = rw->control;
+	uint32_t epoch_id = cmd->res1 >> 32;
+	int stream_id = cmd->res1 & 0xFFFFFFFF;
+	int barrier_flag = (control & (0x1<<9)) >> 9;
+
 // 08-Jan-2018: Added by Jinsoo Yoo
 #ifdef VSSIM_NVME
    event_queue_entry* vssim_event = NULL; 
@@ -298,11 +306,16 @@ static uint16_t nvme_rw(NvmeCtrl *n, NvmeNamespace *ns, NvmeCmd *cmd,
 
 // 18-Sep-2017: Added by Jinsoo Yoo
 #ifdef VSSIM_NVME
+	/* Jieun 20.11.06 */
+	printf("Stream id: %d\t Epoch id: %ld\t Barrier: %d\t", stream_id, epoch_id, barrier_flag);
     if(is_write){
-        vssim_event = SSD_NVME_WRITE(slba, nlb, req, nvme_rw_cb);
+		printf("CMD: WRITE\n");	
+        //vssim_event = SSD_NVME_WRITE(slba, nlb, req, nvme_rw_cb);
+        vssim_event = SSD_NVME_WRITE_BARRIER(slba, nlb, req, nvme_rw_cb, stream_id, epoch_id, barrier_flag); //Jieun add
         req->aiocb = dma_blk_write(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
                       nvme_rw_cb, req);
     }else{
+		printf("CMD: READ\n");
         vssim_event = SSD_NVME_READ(slba, nlb, req, nvme_rw_cb);
         req->aiocb = dma_blk_read(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
                      nvme_rw_cb, req);
diff --git RAMDISK/ram_mount.sh RAMDISK/ram_mount.sh
index 6728247..6de833f 100755
--- RAMDISK/ram_mount.sh
+++ RAMDISK/ram_mount.sh
@@ -7,4 +7,4 @@
 
 mkdir mnt
 chmod 0755 mnt
-sudo mount -t tmpfs -o size=16g tmpfs ./mnt
+sudo mount -t tmpfs -o size=40g tmpfs ./mnt
diff --git vssim_rerun.sh vssim_rerun.sh
index 17301d8..32e0b78 100755
--- vssim_rerun.sh
+++ vssim_rerun.sh
@@ -8,7 +8,7 @@
 #!/bin/bash
 
 MNT="./RAMDISK/mnt"
-QEMU_RUN_OPTION="-m 2048 -enable-kvm -vga cirrus -device nvme,drive=nvme1,serial=foo"
+QEMU_RUN_OPTION="-m 2048 -enable-kvm -vga cirrus -smp 8 -device nvme,drive=nvme1,serial=foo"
 QEMU_NETWORK_OPTION="-net nic,model=virtio -net bridge,br=br0"
 QEMU_IMG1="ssd_hda.img"
 QEMU_IMG2="ssd_nvme.img"
diff --git vssim_run.sh vssim_run.sh
index d0e610a..9674b29 100755
--- vssim_run.sh
+++ vssim_run.sh
@@ -19,8 +19,8 @@ OS_IMG="ubuntu-14.04.4-desktop-amd64.iso"
 sudo rm ./META/*.dat
 
 # Create QEMU disk
-./QEMU/qemu-img create -f qcow2 ${MNT}/${QEMU_IMG1} 8G
-./QEMU/qemu-img create -f qcow2 ${MNT}/${QEMU_IMG2} 8G
+./QEMU/qemu-img create -f qcow2 ${MNT}/${QEMU_IMG1} 50G
+./QEMU/qemu-img create -f qcow2 ${MNT}/${QEMU_IMG2} 10G
 
 # Run VSSIM
 sudo ${QEMU_DIR}/qemu-system-x86_64 -hda ${MNT}/${QEMU_IMG1} -drive file=${MNT}/${QEMU_IMG2},if=none,id=nvme1 -cdrom ${OS_DIR}/${OS_IMG} ${QEMU_RUN_OPTION}
