diff --git CONFIG/ssd.conf CONFIG/ssd.conf
index c7e144b..8d01574 100644
--- CONFIG/ssd.conf
+++ CONFIG/ssd.conf
@@ -1,6 +1,6 @@
 FILE_NAME_HDA			../../RAMDISK/mnt/ssd_hda.img
 
-N_CORES				3
+N_CORES				2
 BACKGROUND_GC_ENABLE		0
 
 PAGE_SIZE			16384
diff --git FIRMWARE/firm_buffer_manager.c FIRMWARE/firm_buffer_manager.c
index 6e27fe8..ca5efe1 100644
--- FIRMWARE/firm_buffer_manager.c
+++ FIRMWARE/firm_buffer_manager.c
@@ -696,6 +696,58 @@ event_queue_entry* CREATE_NEW_EVENT(int io_type, uint64_t slba, uint32_t nlb, vo
 {
 	static uint64_t seq_nb = 0;
 
+	// Allocate new event entry 
+	event_queue_entry* new_eq_entry = calloc(1, sizeof(event_queue_entry));
+	if(new_eq_entry == NULL){
+		printf("[%s] Allocation new event fail.\n", __FUNCTION__);
+		return NULL;
+	}
+
+	// Allocate sequence number for this event 
+	new_eq_entry->seq_nb = seq_nb;
+	seq_nb++;
+
+	// Initialize new event 
+	new_eq_entry->io_type = io_type;
+	new_eq_entry->valid = VALID;
+	new_eq_entry->sector_nb = slba;
+	new_eq_entry->length = nlb;
+	new_eq_entry->cb = cb;
+	new_eq_entry->opaque = opaque;
+	new_eq_entry->buf = NULL;
+	new_eq_entry->n_child = 0;
+	new_eq_entry->n_completed = 0;
+	new_eq_entry->n_trimmed = 0;
+	new_eq_entry->e_state = WAIT_CHILD;
+	
+	/* Jieun add 20.11.24 */
+	new_eq_entry->stream_id = 0;
+	new_eq_entry->epoch_id = 0;
+	new_eq_entry->barrier_flag = 0;
+
+	pthread_mutex_init(&new_eq_entry->lock, NULL);
+	new_eq_entry->flush = false;
+
+	new_eq_entry->t_start = get_usec();
+	new_eq_entry->n_pages = 0;
+
+	new_eq_entry->prev = NULL;
+	new_eq_entry->next = NULL;
+
+	return new_eq_entry;
+}
+
+
+
+/* Jieun add 20.11.24 
+   This function is for barrier-enabled NVMe FTL 
+   Every event queue entry has stream id, epoch id and barrier flag. 
+   If the request is not a order-preserving write request, the stream id, epoch id, and barrier flag will be zero. 
+   */
+event_queue_entry* CREATE_NEW_EVENT_BARRIER(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb, int stream_id, uint32_t epoch_id, int barrier_flag)
+{
+	static uint64_t seq_nb = 0;
+
 	/* Allocate new event entry */
 	event_queue_entry* new_eq_entry = calloc(1, sizeof(event_queue_entry));
 	if(new_eq_entry == NULL){
@@ -720,6 +772,12 @@ event_queue_entry* CREATE_NEW_EVENT(int io_type, uint64_t slba, uint32_t nlb, vo
 	new_eq_entry->n_trimmed = 0;
 	new_eq_entry->e_state = WAIT_CHILD;
 
+	/* Jieun add 20.11.24 */
+
+	new_eq_entry->stream_id = stream_id;
+	new_eq_entry->epoch_id = epoch_id;
+	new_eq_entry->barrier_flag = barrier_flag;
+	
 	pthread_mutex_init(&new_eq_entry->lock, NULL);
 	new_eq_entry->flush = false;
 
@@ -731,7 +789,6 @@ event_queue_entry* CREATE_NEW_EVENT(int io_type, uint64_t slba, uint32_t nlb, vo
 
 	return new_eq_entry;
 }
-
 /* This function should be called after eq_entry->lock is already held. */
 void UPDATE_EVENT_STATE(event_queue_entry* eq_entry, enum event_state state)
 {
diff --git FIRMWARE/firm_buffer_manager.h FIRMWARE/firm_buffer_manager.h
index abb27bc..837ea96 100644
--- FIRMWARE/firm_buffer_manager.h
+++ FIRMWARE/firm_buffer_manager.h
@@ -16,6 +16,15 @@ extern pthread_mutex_t cq_lock;
 
 typedef void CallbackFunc(void *opaque, int ret);
 
+/* Jieun add */
+enum epoch_state{
+	INCOMPLETE = 0,
+	WITHHOLD,
+	UPDATE_QUEUE,
+	COMPLETE,
+};
+
+
 enum vssim_io_type{
 	NOOP = 0,
 	READ,
@@ -82,6 +91,11 @@ typedef struct event_queue_entry
 	/* Bandwidth */
 	int64_t t_start;
 	uint32_t n_pages;
+	
+	/* Jieun add 20.11.24 */
+	int stream_id;
+	uint32_t epoch_id;
+	int barrier_flag;
 
 	/* pointers for candidate queue */
 	struct event_queue_entry* prev;
@@ -105,6 +119,8 @@ event_queue_entry* DEQUEUE_IO(void);
 /* Manipulate event queue entries */
 event_queue_entry* CREATE_NEW_EVENT(int io_type, uint64_t slba, 
 			uint32_t nlb, void* opaque, CallbackFunc *cb);
+
+event_queue_entry* CREATE_NEW_EVENT_BARRIER(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb, int stream_id, uint32_t epoch_id, int barrier_flag); //Jieun add 20.11.24
 void UPDATE_EVENT_STATE(event_queue_entry* eq_entry, enum event_state state); 
 int GET_EVENT_STATE(event_queue_entry* eq_entry); 
 int GET_N_IO_PAGES(uint64_t sector_nb, uint32_t length);
diff --git FIRMWARE/ssd.c FIRMWARE/ssd.c
index 005d8d8..0997c26 100644
--- FIRMWARE/ssd.c
+++ FIRMWARE/ssd.c
@@ -51,9 +51,10 @@ event_queue_entry* SSD_NVME_READ(uint64_t slba, uint32_t nlb, NvmeRequest *req,
 		void(*cb)(void *opaque, int ret))
 {
 	return SSD_RW(READ, slba, nlb, req, cb);
-}
 
+}
 
+/*
 event_queue_entry* SSD_NVME_WRITE(uint64_t slba, uint32_t nlb, NvmeRequest *req,
 		void(*cb)(void *opaque, int ret))
 {
@@ -64,6 +65,20 @@ event_queue_entry* SSD_NVME_WRITE(uint64_t slba, uint32_t nlb, NvmeRequest *req,
 
 	return SSD_RW(WRITE, slba, nlb, req, cb);
 }
+*/
+
+/* Jieun add 20.11.24 */
+event_queue_entry* SSD_NVME_WRITE_BARRIER(uint64_t slba, uint32_t nlb, NvmeRequest *req,
+		void(*cb)(void *opaque, int ret), int stream_id, uint32_t epoch_id, int barrier_flag)
+{
+	if(nlb > N_WB_SECTORS){
+		printf("ERROR[%s] the size of the write event (%u) exceeds the write buffer (%d), please increase the write buffer size in the ssd configuration \n", __FUNCTION__, nlb, N_WB_SECTORS);
+		return NULL;
+	}
+
+//	return SSD_RW(WRITE, slba, nlb, req, cb);
+	return SSD_RW_BARRIER(WRITE, slba, nlb, req, cb, stream_id, epoch_id, barrier_flag); //Jieun add 20.11.24
+}
 
 
 event_queue_entry* SSD_NVME_FLUSH(uint64_t slba, uint32_t nlb, NvmeRequest *req,
@@ -100,6 +115,25 @@ event_queue_entry* SSD_RW(int io_type, uint64_t slba, uint32_t nlb, void* opaque
 	return new_eq_entry;
 }
 
+/* Jieun add 20.11.24 *
+   This function is for barrier-enabled NVMe FTL.
+   SSD_WRITE make a event queue entry which includes stream id, epoch id and barrier flag from Host. */
+event_queue_entry* SSD_RW_BARRIER(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb,  int stream_id, uint32_t epoch_id, int barrier_flag)
+{
+	event_queue_entry* new_eq_entry = NULL;
+
+	/* Create new I/O event */
+	new_eq_entry = CREATE_NEW_EVENT_BARRIER(io_type, slba, nlb, opaque, cb, stream_id, epoch_id, barrier_flag);
+
+	/* Insert new I/O event to the event queue*/
+	ENQUEUE_IO(new_eq_entry);
+
+	/* Wake up the firmware io buffer thread */
+	pthread_cond_signal(&eq_ready);
+
+	return new_eq_entry;
+}
+
 
 void SSD_DSM_DISCARD(NvmeRequest *req, uint32_t nr)
 {
diff --git FIRMWARE/ssd.h FIRMWARE/ssd.h
index ded0310..0285ca4 100644
--- FIRMWARE/ssd.h
+++ FIRMWARE/ssd.h
@@ -28,6 +28,12 @@ event_queue_entry* SSD_NVME_FLUSH(uint64_t slba, uint32_t nlb, NvmeRequest *req,
 
 event_queue_entry* SSD_RW(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb);
 
+/* Jieun add */
+event_queue_entry* SSD_RW_BARRIER(int io_type, uint64_t slba, uint32_t nlb, void* opaque, CallbackFunc *cb,  int stream_id, uint32_t epoch_id, int barrier_flag);
+event_queue_entry* SSD_NVME_WRITE_BARRIER(uint64_t slba, uint32_t nlb, NvmeRequest *req, void(*cb)(void *opaque, int ret), int stream_id, uint32_t epoch_id, int barrier_flag);
+
+
+
 /* TRIM command support */
 void SSD_DSM_DISCARD(NvmeRequest *req, uint32_t nr);
 int IS_SSD_TRIM_ENABLED(void);
diff --git FIRMWARE/vssim_core.c FIRMWARE/vssim_core.c
index 46d86f0..f61cd63 100644
--- FIRMWARE/vssim_core.c
+++ FIRMWARE/vssim_core.c
@@ -17,6 +17,12 @@ pthread_cond_t eq_ready = PTHREAD_COND_INITIALIZER;
 pthread_cond_t* ssd_io_ready; 
 pthread_mutex_t* ssd_io_lock;
 
+/* Jieun add */
+#define N_STREAM 100
+#define N_EPOCH 256
+
+stream_entry* s_table;
+
 FILE* fp_gc_info;	
 
 void MAKE_TIMEOUT(struct timespec *tsp, long timeout_usec)
@@ -36,6 +42,29 @@ void MAKE_TIMEOUT(struct timespec *tsp, long timeout_usec)
 	tsp->tv_nsec = t_usec * 1000;
 }
 
+/* Jieun add */
+void INIT_STREAM_TABLE(void)
+{
+	s_table = (stream_entry*)calloc(sizeof(stream_entry), N_STREAM);
+
+	int i, j;
+	for(i=0;i<N_STREAM;i++){
+		s_table[i].e_table = (epoch_entry*)calloc(sizeof(epoch_entry), N_EPOCH);
+		s_table[i].sid = -1;
+	}
+
+	for(i=0;i<N_STREAM;i++){
+		for(j=0;j<N_EPOCH;j++){
+			s_table[i].e_table[j].eid = -1;
+			s_table[i].e_table[j].state = INCOMPLETE;
+			s_table[i].e_table[j].weid = -1;
+			s_table[i].e_table[j].m_queue.head = NULL;
+			s_table[i].e_table[j].m_queue.tail = NULL;
+			s_table[i].e_table[j].m_queue.n_entry = 0;
+		}
+	}
+}
+
 void INIT_VSSIM_CORE(void)
 {
 	int i, j;
@@ -62,7 +91,6 @@ void INIT_VSSIM_CORE(void)
 		for(i=0; i<N_IO_CORES; i++){
 			/* Init read queue */
 			INIT_PER_CORE_REQUEST_QUEUE(&vs_core[i].read_queue);
-
 			/* Create write queue */
 			vs_core[i].write_queue =
 				(core_req_queue*)calloc(sizeof(core_req_queue), N_WRITE_BUF);
@@ -78,7 +106,11 @@ void INIT_VSSIM_CORE(void)
 
 			/* Init discard queue */
 			INIT_PER_CORE_REQUEST_QUEUE(&vs_core[i].discard_queue);
-
+			
+			/* Jieun add */
+			INIT_PER_CORE_REQUEST_QUEUE(&vs_core[i].withhold_queue);
+			INIT_PER_CORE_REQUEST_QUEUE(&vs_core[i].update_queue);
+			
 			/* Init flash list */
 			INIT_FLASH_LIST(i);
 			vs_core[i].flash_index = i;
@@ -281,7 +313,57 @@ void TERM_VSSIM_CORE(void)
 	fclose(fp_gc_info);
 #endif
 }
+void DELETE_MAPPING_QUEUE(mapping_queue m_queue){
+	int n_entry = m_queue.n_entry;
+	mapping_entry* cur_entry;
+	while(n_entry != 0){
+		cur_entry = m_queue.head;
+		m_queue.head = m_queue.head->next;
+		free(cur_entry);
+		n_entry --;
+	}
+}
+void DELETE_EPOCH_TABLE(uint32_t sidx){
+	epoch_entry* cur_table = s_table[sidx].e_table;
+	int i;
+	for(i=0;i<N_EPOCH;i++){
+		cur_table[i].eid = -1;
+		cur_table[i].state = INCOMPLETE;
+		cur_table[i].weid = -1;
+		if(cur_table[i].m_queue.n_entry){
+			DELETE_MAPPING_QUEUE(cur_table[i].m_queue);
+		}
+		cur_table[i].m_queue.head = NULL;
+		cur_table[i].m_queue.tail = NULL;
+		cur_table[i].m_queue.n_entry = 0;
+	}
+}
+/* Jieun add */
+enum epoch_state SEARCH_EPOCH_STATE(uint32_t sid, uint32_t eid){
+	
+	int s_index = sid % N_STREAM;
+
+	if(s_table[s_index].sid == -1){ //First Access
+		s_table[s_index].sid = sid;
+		if(eid == 0){
+			return COMPLETE;
+		}
+		else{
+			return INCOMPLETE;
+		}
+	}
+	uint32_t e_index = eid % N_EPOCH;
+	printf("Get epoch table\n");	
+	epoch_entry* cur_e_table = s_table[s_index].e_table;
+	
+	int b_index = e_index - 1;
+	if(b_index < 0){
+		b_index = 0;
+	}
+	enum epoch_state cur_state = cur_e_table[b_index].state;
 
+	return cur_state;	
+}
 
 void *FIRM_IO_BUF_THREAD_MAIN_LOOP(void *arg)
 {
@@ -309,6 +391,8 @@ void *FIRM_IO_BUF_THREAD_MAIN_LOOP(void *arg)
 
 		/* Get new IO event */
 		cur_entry = DEQUEUE_IO();
+		// Jieun add for debugging
+	//	printf("FIRM_IO_BUF_THREAD_MAIN_LOOP!!! stream_id: %d\t epoch id: %d\t barrier: %d\n", cur_entry->stream_id, cur_entry->epoch_id, cur_entry->barrier_flag);
 		
 		pthread_mutex_unlock(&eq_lock);
 
@@ -507,6 +591,10 @@ void MERGE_CORE_REQ_ENTRY(core_req_entry* dst_entry, core_req_entry* src_entry)
 	dst_entry->merged_entries.entry_nb++;
 }
 
+/* Jieun add*/
+//void INSERT_REQ_ENTRY_WITTHOLD_QUEUE(core_req_entry* cr_entry){
+
+//}
 void INSERT_NEW_PER_CORE_REQUEST(int core_id, event_queue_entry* eq_entry, 
 			uint64_t sector_nb, uint32_t length, int w_buf_index)
 {
@@ -515,10 +603,22 @@ void INSERT_NEW_PER_CORE_REQUEST(int core_id, event_queue_entry* eq_entry,
 
 	core_req_entry* new_cr_entry = NULL;
 	core_req_queue* cur_cr_queue = NULL;	
+	
+	/* Jieun add */
+	enum epoch_state cur_state;
 
 	/* Get per-core request queue */
 	if(io_type == WRITE){
-		cur_cr_queue = &vs_core[core_id].write_queue[w_buf_index];
+		if(eq_entry->stream_id == 0){
+			cur_cr_queue = &vs_core[core_id].write_queue[w_buf_index];
+		}
+		else{
+			cur_state = SEARCH_EPOCH_STATE(eq_entry->stream_id, eq_entry->epoch_id);
+			printf("cur state:%d\n", cur_state);
+			printf("stream table check:sid %d\n", s_table[eq_entry->stream_id % N_STREAM].sid);
+
+			cur_cr_queue = &vs_core[core_id].write_queue[w_buf_index];
+		}
 	}
 	else if(io_type == READ){
 		cur_cr_queue = &vs_core[core_id].read_queue;
@@ -534,11 +634,17 @@ void INSERT_NEW_PER_CORE_REQUEST(int core_id, event_queue_entry* eq_entry,
 
 	/* Acquire lock for per-core request queue */
 	pthread_mutex_lock(&cur_cr_queue->lock);
-
+	
 	/* Create core request entry */
 	new_cr_entry = CREATE_NEW_CORE_EVENT(eq_entry, core_id, 
 					sector_nb, length, flush);
-
+	/* Jieun add */
+	if(eq_entry->stream_id != 0){
+		new_cr_entry->stream_id = eq_entry->stream_id;
+		new_cr_entry->epoch_id = eq_entry->epoch_id;
+		new_cr_entry->barrier = eq_entry->barrier_flag;
+	}		
+	
 	if(cur_cr_queue->entry_nb == 0){
 		cur_cr_queue->head = new_cr_entry;
 		cur_cr_queue->tail = new_cr_entry;
@@ -814,6 +920,13 @@ core_req_entry* CREATE_NEW_CORE_EVENT(event_queue_entry* eq_entry,
 	new_cr_entry->merged_entries.tail = NULL;
 	pthread_mutex_init(&new_cr_entry->merged_entries.lock, NULL);
 
+	/* Jieun add */
+	new_cr_entry->stream_id = 0;
+	new_cr_entry->epoch_id = 0;
+	new_cr_entry->barrier = 0;
+	
+	new_cr_entry->lpn = -1;
+	new_cr_entry->ppn.addr = -1;
 	return new_cr_entry;
 }
 
diff --git FIRMWARE/vssim_core.h FIRMWARE/vssim_core.h
index 2922939..49840ca 100644
--- FIRMWARE/vssim_core.h
+++ FIRMWARE/vssim_core.h
@@ -17,6 +17,24 @@ extern pthread_cond_t* ssd_io_ready;
 
 typedef void CallbackFunc(void *opaque, int ret);
 
+/* Jieun add */
+typedef struct{
+	uint32_t sid;
+	int eid;
+	int barrier;
+	int lpn;
+	ppn_t ppn;
+	struct mapping_entry* prev;
+	struct mapping_entry* next;
+}mapping_entry;
+
+
+/* jieun add */
+typedef struct{
+	mapping_entry* head;
+	mapping_entry* tail;
+	int n_entry;
+}mapping_queue;
 
 typedef struct core_req_queue
 {
@@ -51,6 +69,13 @@ typedef struct core_req_entry
 
 	/* list for merged entries */
 	core_req_queue merged_entries;
+	
+	/* Jieun add */
+	uint32_t stream_id;
+	int barrier;
+	uint32_t epoch_id;
+	ppn_t ppn;
+	int lpn;
 
 }core_req_entry;
 
@@ -60,6 +85,9 @@ typedef struct vssim_core
 	core_req_queue read_queue;
 	core_req_queue* write_queue;
 	core_req_queue discard_queue;
+	
+	core_req_queue withhold_queue; //Jieun add
+	core_req_queue update_queue; //Jieun add
 
 	int flash_index;
 	int n_flash;
@@ -100,6 +128,20 @@ struct nvme_dsm_range {
 	uint64_t	slba;
 };
 
+typedef struct{
+	int eid;
+	enum epoch_state state;
+	mapping_queue m_queue;
+	int weid;
+}epoch_entry;
+
+typedef struct{
+	int sid;
+	epoch_entry* e_table;
+}stream_entry;
+
+extern stream_entry* s_table;
+
 void MAKE_TIMEOUT(struct timespec *tsp, long timeout_usec);
 
 /* Initialize vssim core structure */
@@ -147,4 +189,7 @@ void INCREASE_N_BGGC_PLANES(int core_id);
 void DECREASE_N_BGGC_PLANES(int core_id);
 void INCREASE_N_FGGC_PLANES(int core_id);
 void DECREASE_N_FGGC_PLANES(int core_id);
+
+//Jieun add
+void INIT_STREAM_TABLE(void);
 #endif
diff --git FTL/PAGE_MAP/ftl.c FTL/PAGE_MAP/ftl.c
index 3723a97..649fdec 100644
--- FTL/PAGE_MAP/ftl.c
+++ FTL/PAGE_MAP/ftl.c
@@ -39,6 +39,11 @@ void FTL_INIT(void)
 		if(ret == -1) goto fail;
 
 		INIT_VSSIM_CORE();	/* Init Flash -> Init Core */
+		
+		INIT_STREAM_TABLE(); //Jieun add
+		//jieun add
+		printf("Stream table initialize done\n");
+
 
 		ret = INIT_MAPPING_TABLE(ret); /* Init Core -> Init Mapping */
 		if(ret == -1) goto fail;
diff --git QEMU/hw/block/nvme.c QEMU/hw/block/nvme.c
index 7d08f62..d93f04c 100644
--- QEMU/hw/block/nvme.c
+++ QEMU/hw/block/nvme.c
@@ -248,6 +248,8 @@ static uint16_t nvme_flush(NvmeCtrl *n, NvmeNamespace *ns, NvmeCmd *cmd,
          BLOCK_ACCT_FLUSH);
 
 #ifdef VSSIM_NVME
+	/* Jieun 20.11.16 */
+	printf("\n CMD:FLUSH \n");
     event_queue_entry* vssim_event = NULL; 
     vssim_event = SSD_NVME_FLUSH(0, 0, req, nvme_rw_cb);
 
@@ -276,6 +278,12 @@ static uint16_t nvme_rw(NvmeCtrl *n, NvmeNamespace *ns, NvmeCmd *cmd,
     int is_write = rw->opcode == NVME_CMD_WRITE ? 1 : 0;
     enum BlockAcctType acct = is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ;
 
+	/* Jieun 20.11.16 */
+	int control = rw->control;
+	uint32_t epoch_id = cmd->res1 >> 32;
+	int stream_id = cmd->res1 & 0xFFFFFFFF;
+	int barrier_flag = (control & (0x1<<9)) >> 9;
+
 // 08-Jan-2018: Added by Jinsoo Yoo
 #ifdef VSSIM_NVME
    event_queue_entry* vssim_event = NULL; 
@@ -298,11 +306,16 @@ static uint16_t nvme_rw(NvmeCtrl *n, NvmeNamespace *ns, NvmeCmd *cmd,
 
 // 18-Sep-2017: Added by Jinsoo Yoo
 #ifdef VSSIM_NVME
+	/* Jieun 20.11.06 */
+	printf("Stream id: %d\t Epoch id: %ld\t Barrier: %d\t", stream_id, epoch_id, barrier_flag);
     if(is_write){
-        vssim_event = SSD_NVME_WRITE(slba, nlb, req, nvme_rw_cb);
+		printf("CMD: WRITE\n");	
+        //vssim_event = SSD_NVME_WRITE(slba, nlb, req, nvme_rw_cb);
+        vssim_event = SSD_NVME_WRITE_BARRIER(slba, nlb, req, nvme_rw_cb, stream_id, epoch_id, barrier_flag); //Jieun add
         req->aiocb = dma_blk_write(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
                       nvme_rw_cb, req);
     }else{
+		printf("CMD: READ\n");
         vssim_event = SSD_NVME_READ(slba, nlb, req, nvme_rw_cb);
         req->aiocb = dma_blk_read(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
                      nvme_rw_cb, req);
diff --git RAMDISK/ram_mount.sh RAMDISK/ram_mount.sh
index 6728247..6de833f 100755
--- RAMDISK/ram_mount.sh
+++ RAMDISK/ram_mount.sh
@@ -7,4 +7,4 @@
 
 mkdir mnt
 chmod 0755 mnt
-sudo mount -t tmpfs -o size=16g tmpfs ./mnt
+sudo mount -t tmpfs -o size=40g tmpfs ./mnt
diff --git vssim_rerun.sh vssim_rerun.sh
index 17301d8..32e0b78 100755
--- vssim_rerun.sh
+++ vssim_rerun.sh
@@ -8,7 +8,7 @@
 #!/bin/bash
 
 MNT="./RAMDISK/mnt"
-QEMU_RUN_OPTION="-m 2048 -enable-kvm -vga cirrus -device nvme,drive=nvme1,serial=foo"
+QEMU_RUN_OPTION="-m 2048 -enable-kvm -vga cirrus -smp 8 -device nvme,drive=nvme1,serial=foo"
 QEMU_NETWORK_OPTION="-net nic,model=virtio -net bridge,br=br0"
 QEMU_IMG1="ssd_hda.img"
 QEMU_IMG2="ssd_nvme.img"
diff --git vssim_run.sh vssim_run.sh
index d0e610a..9674b29 100755
--- vssim_run.sh
+++ vssim_run.sh
@@ -19,8 +19,8 @@ OS_IMG="ubuntu-14.04.4-desktop-amd64.iso"
 sudo rm ./META/*.dat
 
 # Create QEMU disk
-./QEMU/qemu-img create -f qcow2 ${MNT}/${QEMU_IMG1} 8G
-./QEMU/qemu-img create -f qcow2 ${MNT}/${QEMU_IMG2} 8G
+./QEMU/qemu-img create -f qcow2 ${MNT}/${QEMU_IMG1} 50G
+./QEMU/qemu-img create -f qcow2 ${MNT}/${QEMU_IMG2} 10G
 
 # Run VSSIM
 sudo ${QEMU_DIR}/qemu-system-x86_64 -hda ${MNT}/${QEMU_IMG1} -drive file=${MNT}/${QEMU_IMG2},if=none,id=nvme1 -cdrom ${OS_DIR}/${OS_IMG} ${QEMU_RUN_OPTION}
