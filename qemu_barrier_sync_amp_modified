diff --git block/block-backend.c block/block-backend.c
index 681b240..a481482 100644
--- block/block-backend.c
+++ block/block-backend.c
@@ -1190,6 +1190,72 @@ int coroutine_fn blk_co_pwritev(BlockBackend *blk, int64_t offset,
     return ret;
 }
 
+/* Jieun add */
+int coroutine_fn blk_co_pbarrier_write(BlockBackend *blk, int64_t offset,
+                                unsigned int bytes, QEMUIOVector *qiov,
+                                BdrvRequestFlags flags)
+{
+    int ret;
+    BlockDriverState *bs = blk_bs(blk);
+
+//    trace_blk_co_pwritev(blk, bs, offset, bytes, flags);
+
+    ret = blk_check_byte_request(blk, offset, bytes);
+    if (ret < 0) {
+        return ret;
+    }
+
+    bdrv_inc_in_flight(bs);
+    /* throttling disk I/O */
+    if (blk->public.throttle_group_member.throttle_state) {
+        throttle_group_co_io_limits_intercept(&blk->public.throttle_group_member,
+                bytes, true);
+    }
+
+	//   if (!blk->enable_write_cache) {
+  //      flags |= BDRV_REQ_FUA;
+  //  }
+
+    //ret = bdrv_co_pwritev(blk->root, offset, bytes, qiov, flags);
+   // ret = bdrv_co_pfua(blk->root, offset, bytes, qiov, flags);
+    ret = bdrv_co_pbarrier_write(blk->root, offset, bytes, qiov, flags);
+    bdrv_dec_in_flight(bs);
+    return ret;
+}
+
+
+/* Jieun add */
+int coroutine_fn blk_co_pfua(BlockBackend *blk, int64_t offset,
+                                unsigned int bytes, QEMUIOVector *qiov,
+                                BdrvRequestFlags flags)
+{
+    int ret;
+    BlockDriverState *bs = blk_bs(blk);
+
+//    trace_blk_co_pwritev(blk, bs, offset, bytes, flags);
+
+    ret = blk_check_byte_request(blk, offset, bytes);
+    if (ret < 0) {
+        return ret;
+    }
+
+    bdrv_inc_in_flight(bs);
+    /* throttling disk I/O */
+    if (blk->public.throttle_group_member.throttle_state) {
+        throttle_group_co_io_limits_intercept(&blk->public.throttle_group_member,
+                bytes, true);
+    }
+
+	//   if (!blk->enable_write_cache) {
+  //      flags |= BDRV_REQ_FUA;
+  //  }
+
+    //ret = bdrv_co_pwritev(blk->root, offset, bytes, qiov, flags);
+    ret = bdrv_co_pfua(blk->root, offset, bytes, qiov, flags);
+    bdrv_dec_in_flight(bs);
+    return ret;
+}
+
 typedef struct BlkRwCo {
     BlockBackend *blk;
     int64_t offset;
@@ -1396,6 +1462,7 @@ static void blk_aio_write_entry(void *opaque)
     blk_aio_complete(acb);
 }
 
+
 BlockAIOCB *blk_aio_pwrite_zeroes(BlockBackend *blk, int64_t offset,
                                   int count, BdrvRequestFlags flags,
                                   BlockCompletionFunc *cb, void *opaque)
@@ -1467,6 +1534,51 @@ BlockAIOCB *blk_aio_pwritev(BlockBackend *blk, int64_t offset,
                         blk_aio_write_entry, flags, cb, opaque);
 }
 
+/* Jieun add */
+void blk_aio_barrier_write_entry(void *opaque)
+{
+    BlkAioEmAIOCB *acb = opaque;
+    BlkRwCo *rwco = &acb->rwco;
+    QEMUIOVector *qiov = rwco->iobuf;
+
+    assert(!qiov || qiov->size == acb->bytes);
+    rwco->ret = blk_co_pbarrier_write(rwco->blk, rwco->offset, acb->bytes,
+                               qiov, rwco->flags);
+    blk_aio_complete(acb);
+}
+
+
+/* Jieun add */
+BlockAIOCB *blk_aio_pbarrierwritev(BlockBackend *blk, int64_t offset,
+                            QEMUIOVector *qiov, BdrvRequestFlags flags,
+                            BlockCompletionFunc *cb, void *opaque)
+{
+    return blk_aio_prwv(blk, offset, qiov->size, qiov,
+                        blk_aio_barrier_write_entry, flags, cb, opaque);
+}
+
+/* Jieun add */
+void blk_aio_fua_entry(void *opaque)
+{
+    BlkAioEmAIOCB *acb = opaque;
+    BlkRwCo *rwco = &acb->rwco;
+    QEMUIOVector *qiov = rwco->iobuf;
+
+    assert(!qiov || qiov->size == acb->bytes);
+    rwco->ret = blk_co_pfua(rwco->blk, rwco->offset, acb->bytes,
+                               qiov, rwco->flags);
+    blk_aio_complete(acb);
+}
+
+/* Jieun add */
+BlockAIOCB *blk_aio_pfuav(BlockBackend *blk, int64_t offset,
+                            QEMUIOVector *qiov, BdrvRequestFlags flags,
+                            BlockCompletionFunc *cb, void *opaque)
+{
+    return blk_aio_prwv(blk, offset, qiov->size, qiov,
+                        blk_aio_fua_entry, flags, cb, opaque);
+}
+
 static void blk_aio_flush_entry(void *opaque)
 {
     BlkAioEmAIOCB *acb = opaque;
diff --git block/file-posix.c block/file-posix.c
index 30642b6..3d0acb5 100644
--- block/file-posix.c
+++ block/file-posix.c
@@ -138,7 +138,8 @@ do { \
  * leaving a few more bytes for its future use. */
 #define RAW_LOCK_PERM_BASE             100
 #define RAW_LOCK_SHARED_BASE           200
-
+/* Jieun add */
+//#define DEBUG
 typedef struct BDRVRawState {
     int fd;
     int lock_fd;
@@ -1056,6 +1057,42 @@ static ssize_t handle_aiocb_ioctl(RawPosixAIOData *aiocb)
     return 0;
 }
 
+/* Jieun add */
+static ssize_t handle_aiocb_barrier_write(RawPosixAIOData *aiocb)
+{
+    BDRVRawState *s = aiocb->bs->opaque;
+    int ret;
+
+    if (s->page_cache_inconsistent) {
+        return -EIO;
+    }
+#ifdef DEBUG
+		fprintf(stderr,"[jieun-debug] qemu_fdatasync\n");
+#endif
+
+    ret = qemu_fdatabarrier(aiocb->aio_fildes);
+    if (ret == -1) {
+        /* There is no clear definition of the semantics of a failing fsync(),
+         * so we may have to assume the worst. The sad truth is that this
+         * assumption is correct for Linux. Some pages are now probably marked
+         * clean in the page cache even though they are inconsistent with the
+         * on-disk contents. The next fdatasync() call would succeed, but no
+         * further writeback attempt will be made. We can't get back to a state
+         * in which we know what is on disk (we would have to rewrite
+         * everything that was touched since the last fdatasync() at least), so
+         * make bdrv_flush() fail permanently. Given that the behaviour isn't
+         * really defined, I have little hope that other OSes are doing better.
+         *
+         * Obviously, this doesn't affect O_DIRECT, which bypasses the page
+         * cache. */
+        if ((s->open_flags & O_DIRECT) == 0) {
+            s->page_cache_inconsistent = true;
+        }
+        return -errno;
+    }
+    return 0;
+}
+
 static ssize_t handle_aiocb_flush(RawPosixAIOData *aiocb)
 {
     BDRVRawState *s = aiocb->bs->opaque;
@@ -1064,6 +1101,9 @@ static ssize_t handle_aiocb_flush(RawPosixAIOData *aiocb)
     if (s->page_cache_inconsistent) {
         return -EIO;
     }
+#ifdef DEBUG
+		fprintf(stderr,"[jieun-debug] qemu_fdatasync\n");
+#endif
 
     ret = qemu_fdatasync(aiocb->aio_fildes);
     if (ret == -1) {
@@ -1492,6 +1532,9 @@ static int aio_worker(void *arg)
     case QEMU_AIO_FLUSH:
         ret = handle_aiocb_flush(aiocb);
         break;
+    case QEMU_AIO_BARRIER_WRITE:
+        ret = handle_aiocb_barrier_write(aiocb);
+        break;
     case QEMU_AIO_IOCTL:
         ret = handle_aiocb_ioctl(aiocb);
         break;
@@ -1637,7 +1680,17 @@ static BlockAIOCB *raw_aio_flush(BlockDriverState *bs,
 
     return paio_submit(bs, s->fd, 0, NULL, 0, cb, opaque, QEMU_AIO_FLUSH);
 }
+/* Jieun add */
+static BlockAIOCB *raw_aio_barrier_write(BlockDriverState *bs,
+        BlockCompletionFunc *cb, void *opaque)
+{
+    BDRVRawState *s = bs->opaque;
 
+    if (fd_open(bs) < 0)
+        return NULL;
+
+    return paio_submit(bs, s->fd, 0, NULL, 0, cb, opaque, QEMU_AIO_BARRIER_WRITE);
+}
 static void raw_close(BlockDriverState *bs)
 {
     BDRVRawState *s = bs->opaque;
@@ -2336,6 +2389,7 @@ BlockDriver bdrv_file = {
     .bdrv_co_preadv         = raw_co_preadv,
     .bdrv_co_pwritev        = raw_co_pwritev,
     .bdrv_aio_flush = raw_aio_flush,
+    .bdrv_aio_barrier_write = raw_aio_barrier_write,	//Jieun add
     .bdrv_aio_pdiscard = raw_aio_pdiscard,
     .bdrv_refresh_limits = raw_refresh_limits,
     .bdrv_io_plug = raw_aio_plug,
diff --git block/io.c block/io.c
index bd9a19a..49a568b 100644
--- block/io.c
+++ block/io.c
@@ -1792,6 +1792,284 @@ out:
     return ret;
 }
 
+/* Jieun add */
+int coroutine_fn bdrv_co_pbarrier_write(BdrvChild *child,
+    int64_t offset, unsigned int bytes, QEMUIOVector *qiov,
+    BdrvRequestFlags flags)
+{
+    BlockDriverState *bs = child->bs;
+    BdrvTrackedRequest req;
+    uint64_t align = bs->bl.request_alignment;
+    uint8_t *head_buf = NULL;
+    uint8_t *tail_buf = NULL;
+    QEMUIOVector local_qiov;
+    bool use_local_qiov = false;
+    int ret;
+		
+    //trace_bdrv_co_pwritev(child->bs, offset, bytes, flags);
+
+    if (!bs->drv) {
+        return -ENOMEDIUM;
+    }
+    if (bs->read_only) {
+        return -EPERM;
+    }
+    assert(!(bs->open_flags & BDRV_O_INACTIVE));
+
+    ret = bdrv_check_byte_request(bs, offset, bytes);
+    if (ret < 0) {
+        return ret;
+    }
+		
+    bdrv_inc_in_flight(bs);
+    /*
+     * Align write if necessary by performing a read-modify-write cycle.
+     * Pad qiov with the read parts and be sure to have a tracked request not
+     * only for bdrv_aligned_pwritev, but also for the reads of the RMW cycle.
+     */
+    tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_WRITE);
+
+    if (flags & BDRV_REQ_ZERO_WRITE) {
+        ret = bdrv_co_do_zero_pwritev(child, offset, bytes, flags, &req);
+        goto out;
+    }
+
+    if (offset & (align - 1)) {
+        QEMUIOVector head_qiov;
+        struct iovec head_iov;
+
+        mark_request_serialising(&req, align);
+        wait_serialising_requests(&req);
+
+        head_buf = qemu_blockalign(bs, align);
+        head_iov = (struct iovec) {
+            .iov_base   = head_buf,
+            .iov_len    = align,
+        };
+        qemu_iovec_init_external(&head_qiov, &head_iov, 1);
+
+        bdrv_debug_event(bs, BLKDBG_PWRITEV_RMW_HEAD);
+        ret = bdrv_aligned_preadv(child, &req, offset & ~(align - 1), align,
+                                  align, &head_qiov, 0);
+        if (ret < 0) {
+            goto fail;
+        }
+        bdrv_debug_event(bs, BLKDBG_PWRITEV_RMW_AFTER_HEAD);
+
+        qemu_iovec_init(&local_qiov, qiov->niov + 2);
+        qemu_iovec_add(&local_qiov, head_buf, offset & (align - 1));
+        qemu_iovec_concat(&local_qiov, qiov, 0, qiov->size);
+        use_local_qiov = true;
+
+        bytes += offset & (align - 1);
+        offset = offset & ~(align - 1);
+
+        /* We have read the tail already if the request is smaller
+         * than one aligned block.
+         */
+        if (bytes < align) {
+            qemu_iovec_add(&local_qiov, head_buf + bytes, align - bytes);
+            bytes = align;
+        }
+    }
+
+    if ((offset + bytes) & (align - 1)) {
+        QEMUIOVector tail_qiov;
+        struct iovec tail_iov;
+        size_t tail_bytes;
+        bool waited;
+
+        mark_request_serialising(&req, align);
+        waited = wait_serialising_requests(&req);
+        assert(!waited || !use_local_qiov);
+
+        tail_buf = qemu_blockalign(bs, align);
+        tail_iov = (struct iovec) {
+            .iov_base   = tail_buf,
+            .iov_len    = align,
+        };
+        qemu_iovec_init_external(&tail_qiov, &tail_iov, 1);
+
+        bdrv_debug_event(bs, BLKDBG_PWRITEV_RMW_TAIL);
+        ret = bdrv_aligned_preadv(child, &req, (offset + bytes) & ~(align - 1),
+                                  align, align, &tail_qiov, 0);
+        if (ret < 0) {
+            goto fail;
+        }
+        bdrv_debug_event(bs, BLKDBG_PWRITEV_RMW_AFTER_TAIL);
+
+        if (!use_local_qiov) {
+            qemu_iovec_init(&local_qiov, qiov->niov + 1);
+            qemu_iovec_concat(&local_qiov, qiov, 0, qiov->size);
+            use_local_qiov = true;
+        }
+
+        tail_bytes = (offset + bytes) & (align - 1);
+        qemu_iovec_add(&local_qiov, tail_buf + tail_bytes, align - tail_bytes);
+
+        bytes = ROUND_UP(bytes, align);
+    }
+
+    ret = bdrv_aligned_pwritev(child, &req, offset, bytes, align,
+                               use_local_qiov ? &local_qiov : qiov,
+                               flags);
+		/* Jieun add */	
+    //ret = bdrv_co_flush(bs);
+    ret = bdrv_co_barrier_write(bs);
+
+fail:
+
+    if (use_local_qiov) {
+        qemu_iovec_destroy(&local_qiov);
+    }
+    qemu_vfree(head_buf);
+    qemu_vfree(tail_buf);
+out:
+    tracked_request_end(&req);
+    bdrv_dec_in_flight(bs);
+    return ret;
+}
+
+
+
+/* Jieun add */
+int coroutine_fn bdrv_co_pfua(BdrvChild *child,
+    int64_t offset, unsigned int bytes, QEMUIOVector *qiov,
+    BdrvRequestFlags flags)
+{
+    BlockDriverState *bs = child->bs;
+    BdrvTrackedRequest req;
+    uint64_t align = bs->bl.request_alignment;
+    uint8_t *head_buf = NULL;
+    uint8_t *tail_buf = NULL;
+    QEMUIOVector local_qiov;
+    bool use_local_qiov = false;
+    int ret;
+
+    trace_bdrv_co_pwritev(child->bs, offset, bytes, flags);
+
+    if (!bs->drv) {
+        return -ENOMEDIUM;
+    }
+    if (bs->read_only) {
+        return -EPERM;
+    }
+    assert(!(bs->open_flags & BDRV_O_INACTIVE));
+
+    ret = bdrv_check_byte_request(bs, offset, bytes);
+    if (ret < 0) {
+        return ret;
+    }
+
+    bdrv_inc_in_flight(bs);
+    /*
+     * Align write if necessary by performing a read-modify-write cycle.
+     * Pad qiov with the read parts and be sure to have a tracked request not
+     * only for bdrv_aligned_pwritev, but also for the reads of the RMW cycle.
+     */
+    tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_WRITE);
+
+    if (flags & BDRV_REQ_ZERO_WRITE) {
+        ret = bdrv_co_do_zero_pwritev(child, offset, bytes, flags, &req);
+        goto out;
+    }
+
+    if (offset & (align - 1)) {
+        QEMUIOVector head_qiov;
+        struct iovec head_iov;
+
+        mark_request_serialising(&req, align);
+        wait_serialising_requests(&req);
+
+        head_buf = qemu_blockalign(bs, align);
+        head_iov = (struct iovec) {
+            .iov_base   = head_buf,
+            .iov_len    = align,
+        };
+        qemu_iovec_init_external(&head_qiov, &head_iov, 1);
+
+        bdrv_debug_event(bs, BLKDBG_PWRITEV_RMW_HEAD);
+        ret = bdrv_aligned_preadv(child, &req, offset & ~(align - 1), align,
+                                  align, &head_qiov, 0);
+        if (ret < 0) {
+            goto fail;
+        }
+        bdrv_debug_event(bs, BLKDBG_PWRITEV_RMW_AFTER_HEAD);
+
+        qemu_iovec_init(&local_qiov, qiov->niov + 2);
+        qemu_iovec_add(&local_qiov, head_buf, offset & (align - 1));
+        qemu_iovec_concat(&local_qiov, qiov, 0, qiov->size);
+        use_local_qiov = true;
+
+        bytes += offset & (align - 1);
+        offset = offset & ~(align - 1);
+
+        /* We have read the tail already if the request is smaller
+         * than one aligned block.
+         */
+        if (bytes < align) {
+            qemu_iovec_add(&local_qiov, head_buf + bytes, align - bytes);
+            bytes = align;
+        }
+    }
+
+    if ((offset + bytes) & (align - 1)) {
+        QEMUIOVector tail_qiov;
+        struct iovec tail_iov;
+        size_t tail_bytes;
+        bool waited;
+
+        mark_request_serialising(&req, align);
+        waited = wait_serialising_requests(&req);
+        assert(!waited || !use_local_qiov);
+
+        tail_buf = qemu_blockalign(bs, align);
+        tail_iov = (struct iovec) {
+            .iov_base   = tail_buf,
+            .iov_len    = align,
+        };
+        qemu_iovec_init_external(&tail_qiov, &tail_iov, 1);
+
+        bdrv_debug_event(bs, BLKDBG_PWRITEV_RMW_TAIL);
+        ret = bdrv_aligned_preadv(child, &req, (offset + bytes) & ~(align - 1),
+                                  align, align, &tail_qiov, 0);
+        if (ret < 0) {
+            goto fail;
+        }
+        bdrv_debug_event(bs, BLKDBG_PWRITEV_RMW_AFTER_TAIL);
+
+        if (!use_local_qiov) {
+            qemu_iovec_init(&local_qiov, qiov->niov + 1);
+            qemu_iovec_concat(&local_qiov, qiov, 0, qiov->size);
+            use_local_qiov = true;
+        }
+
+        tail_bytes = (offset + bytes) & (align - 1);
+        qemu_iovec_add(&local_qiov, tail_buf + tail_bytes, align - tail_bytes);
+
+        bytes = ROUND_UP(bytes, align);
+    }
+
+    ret = bdrv_aligned_pwritev(child, &req, offset, bytes, align,
+                               use_local_qiov ? &local_qiov : qiov,
+                               flags);
+		/* Jieun add */	
+    ret = bdrv_co_flush(bs);
+
+fail:
+
+    if (use_local_qiov) {
+        qemu_iovec_destroy(&local_qiov);
+    }
+    qemu_vfree(head_buf);
+    qemu_vfree(tail_buf);
+out:
+    tracked_request_end(&req);
+    bdrv_dec_in_flight(bs);
+    return ret;
+}
+
+
 static int coroutine_fn bdrv_co_do_writev(BdrvChild *child,
     int64_t sector_num, int nb_sectors, QEMUIOVector *qiov,
     BdrvRequestFlags flags)
@@ -2388,6 +2666,14 @@ typedef struct FlushCo {
     int ret;
 } FlushCo;
 
+/* Jieun add */
+static void coroutine_fn bdrv_barrier_write_co_entry(void *opaque)
+{
+    FlushCo *rwco = opaque;
+
+    //rwco->ret = bdrv_co_flush(rwco->bs);
+    rwco->ret = bdrv_co_barrier_write(rwco->bs);
+}
 
 static void coroutine_fn bdrv_flush_co_entry(void *opaque)
 {
@@ -2396,6 +2682,120 @@ static void coroutine_fn bdrv_flush_co_entry(void *opaque)
     rwco->ret = bdrv_co_flush(rwco->bs);
 }
 
+/* Jieun add */
+int coroutine_fn bdrv_co_barrier_write(BlockDriverState *bs)
+{
+    int current_gen;
+    int ret = 0;
+
+    bdrv_inc_in_flight(bs);
+
+    if (!bdrv_is_inserted(bs) || bdrv_is_read_only(bs) ||
+        bdrv_is_sg(bs)) {
+        goto early_exit;
+    }
+
+    qemu_co_mutex_lock(&bs->reqs_lock);
+    current_gen = atomic_read(&bs->write_gen);
+
+    /* Wait until any previous flushes are completed */
+    while (bs->active_flush_req) {
+        qemu_co_queue_wait(&bs->flush_queue, &bs->reqs_lock);
+    }
+
+    /* Flushes reach this point in nondecreasing current_gen order.  */
+    bs->active_flush_req = true;
+    qemu_co_mutex_unlock(&bs->reqs_lock);
+
+    /* Write back all layers by calling one driver function */
+    if (bs->drv->bdrv_co_flush) {
+        ret = bs->drv->bdrv_co_flush(bs);
+        goto out;
+    }
+
+    /* Write back cached data to the OS even with cache=unsafe */
+    BLKDBG_EVENT(bs->file, BLKDBG_FLUSH_TO_OS);
+    if (bs->drv->bdrv_co_flush_to_os) {
+        ret = bs->drv->bdrv_co_flush_to_os(bs);
+        if (ret < 0) {
+            goto out;
+        }
+    }
+
+    /* But don't actually force it to the disk with cache=unsafe */
+    if (bs->open_flags & BDRV_O_NO_FLUSH) {
+        goto flush_parent;
+    }
+
+    /* Check if we really need to flush anything */
+    if (bs->flushed_gen == current_gen) {
+        goto flush_parent;
+    }
+
+    BLKDBG_EVENT(bs->file, BLKDBG_FLUSH_TO_DISK);
+    if (!bs->drv) {
+        /* bs->drv->bdrv_co_flush() might have ejected the BDS
+         * (even in case of apparent success) */
+        ret = -ENOMEDIUM;
+        goto out;
+    }
+    if (bs->drv->bdrv_co_flush_to_disk) {
+        ret = bs->drv->bdrv_co_flush_to_disk(bs);
+    } else if (bs->drv->bdrv_aio_barrier_write) {
+        BlockAIOCB *acb;
+        CoroutineIOCompletion co = {
+            .coroutine = qemu_coroutine_self(),
+        };
+
+        acb = bs->drv->bdrv_aio_barrier_write(bs, bdrv_co_io_em_complete, &co);
+        if (acb == NULL) {
+            ret = -EIO;
+        } else {
+            qemu_coroutine_yield();
+            ret = co.ret;
+        }
+    } else {
+        /*
+         * Some block drivers always operate in either writethrough or unsafe
+         * mode and don't support bdrv_flush therefore. Usually qemu doesn't
+         * know how the server works (because the behaviour is hardcoded or
+         * depends on server-side configuration), so we can't ensure that
+         * everything is safe on disk. Returning an error doesn't work because
+         * that would break guests even if the server operates in writethrough
+         * mode.
+         *
+         * Let's hope the user knows what he's doing.
+         */
+        ret = 0;
+    }
+
+    if (ret < 0) {
+        goto out;
+    }
+
+    /* Now flush the underlying protocol.  It will also have BDRV_O_NO_FLUSH
+     * in the case of cache=unsafe, so there are no useless flushes.
+     */
+flush_parent:
+    ret = bs->file ? bdrv_co_barrier_write(bs->file->bs) : 0;
+out:
+    /* Notify any pending flushes that we have completed */
+    if (ret == 0) {
+        bs->flushed_gen = current_gen;
+    }
+
+    qemu_co_mutex_lock(&bs->reqs_lock);
+    bs->active_flush_req = false;
+    /* Return value is ignored - it's ok if wait queue is empty */
+    qemu_co_queue_next(&bs->flush_queue);
+    qemu_co_mutex_unlock(&bs->reqs_lock);
+
+early_exit:
+    bdrv_dec_in_flight(bs);
+    return ret;
+}
+
+
 int coroutine_fn bdrv_co_flush(BlockDriverState *bs)
 {
     int current_gen;
@@ -2508,6 +2908,28 @@ early_exit:
     return ret;
 }
 
+/* Jieun add */
+int bdrv_barrier_write(BlockDriverState *bs)
+{
+    Coroutine *co;
+    FlushCo flush_co = {
+        .bs = bs,
+        .ret = NOT_DONE,
+    };
+
+    if (qemu_in_coroutine()) {
+        /* Fast-path if already in coroutine context */
+        //bdrv_flush_co_entry(&flush_co);
+        bdrv_barrier_write_co_entry(&flush_co);
+    } else {
+        co = qemu_coroutine_create(bdrv_barrier_write_co_entry, &flush_co);
+        bdrv_coroutine_enter(bs, co);
+        BDRV_POLL_WHILE(bs, flush_co.ret == NOT_DONE);
+    }
+
+    return flush_co.ret;
+}
+
 int bdrv_flush(BlockDriverState *bs)
 {
     Coroutine *co;
diff --git block/nvme.c block/nvme.c
index c4f3a7b..79a02c5 100644
--- block/nvme.c
+++ block/nvme.c
@@ -532,6 +532,7 @@ static bool nvme_add_io_queue(BlockDriverState *bs, Error **errp)
     s->queues = g_renew(NVMeQueuePair *, s->queues, n + 1);
     s->queues[n] = q;
     s->nr_queues++;
+	//fprintf(stderr,"[jieun-debug] nr queues %d\n", s->nr_queues);
     return true;
 }
 
diff --git block/qcow2-cache.c block/qcow2-cache.c
index d9dafa3..a68ce13 100644
--- block/qcow2-cache.c
+++ block/qcow2-cache.c
@@ -162,6 +162,22 @@ int qcow2_cache_destroy(Qcow2Cache *c)
 
     return 0;
 }
+/* Jieun add */ 
+static int qcow2_cache_barrier_dependency(BlockDriverState *bs, Qcow2Cache *c)
+{
+    int ret;
+
+    //ret = qcow2_cache_flush(bs, c->depends);
+    ret = qcow2_cache_barrier(bs, c->depends);	//Jieun add
+    if (ret < 0) {
+        return ret;
+    }
+
+    c->depends = NULL;
+    c->depends_on_flush = false;
+
+    return 0;
+}
 
 static int qcow2_cache_flush_dependency(BlockDriverState *bs, Qcow2Cache *c)
 {
@@ -191,7 +207,8 @@ static int qcow2_cache_entry_flush(BlockDriverState *bs, Qcow2Cache *c, int i)
                                   c == s->l2_table_cache, i);
 
     if (c->depends) {
-        ret = qcow2_cache_flush_dependency(bs, c);
+        //ret = qcow2_cache_flush_dependency(bs, c);
+        ret = qcow2_cache_barrier_dependency(bs, c);	//Jieun add
     } else if (c->depends_on_flush) {
         ret = bdrv_flush(bs->file->bs);
         if (ret >= 0) {
@@ -254,6 +271,22 @@ int qcow2_cache_write(BlockDriverState *bs, Qcow2Cache *c)
     return result;
 }
 
+/* Jieun add */
+int qcow2_cache_barrier(BlockDriverState *bs, Qcow2Cache *c)
+{
+    int result = qcow2_cache_write(bs, c);
+
+    if (result == 0) {
+        //int ret = bdrv_flush(bs->file->bs);
+        int ret = bdrv_barrier_write(bs->file->bs);
+        if (ret < 0) {
+            result = ret;
+        }
+    }
+
+    return result;
+}
+
 int qcow2_cache_flush(BlockDriverState *bs, Qcow2Cache *c)
 {
     int result = qcow2_cache_write(bs, c);
diff --git block/qcow2.h block/qcow2.h
index adf5c39..ac2217a 100644
--- block/qcow2.h
+++ block/qcow2.h
@@ -651,6 +651,7 @@ int qcow2_cache_destroy(Qcow2Cache *c);
 
 void qcow2_cache_entry_mark_dirty(Qcow2Cache *c, void *table);
 int qcow2_cache_flush(BlockDriverState *bs, Qcow2Cache *c);
+int qcow2_cache_barrier(BlockDriverState *bs, Qcow2Cache *c); //Jieun add
 int qcow2_cache_write(BlockDriverState *bs, Qcow2Cache *c);
 int qcow2_cache_set_dependency(BlockDriverState *bs, Qcow2Cache *c,
     Qcow2Cache *dependency);
diff --git dma-helpers.c dma-helpers.c
index 2d7e02d..c719e84 100644
--- dma-helpers.c
+++ dma-helpers.c
@@ -16,6 +16,16 @@
 
 /* #define DEBUG_IOMMU */
 
+/* Jieun add */
+BlockAIOCB *dma_blk_fua_io_func(int64_t offset, QEMUIOVector *iov,
+                                  BlockCompletionFunc *cb, void *cb_opaque,
+                                  void *opaque);
+
+/* Jieun add */
+BlockAIOCB *dma_blk_barrier_write_io_func(int64_t offset, QEMUIOVector *iov,
+                                  BlockCompletionFunc *cb, void *cb_opaque,
+                                  void *opaque);
+
 int dma_memory_set(AddressSpace *as, dma_addr_t addr, uint8_t c, dma_addr_t len)
 {
     dma_barrier(as, DMA_DIRECTION_FROM_DEVICE);
@@ -265,6 +275,45 @@ BlockAIOCB *dma_blk_write(BlockBackend *blk,
                       DMA_DIRECTION_TO_DEVICE);
 }
 
+/* Jieun add */
+BlockAIOCB *dma_blk_barrier_write_io_func(int64_t offset, QEMUIOVector *iov,
+                                  BlockCompletionFunc *cb, void *cb_opaque,
+                                  void *opaque)
+{
+    BlockBackend *blk = opaque;
+    return blk_aio_pbarrierwritev(blk, offset, iov, 0, cb, cb_opaque);
+}
+
+/* Jieun add */
+BlockAIOCB *dma_blk_barrier_write(BlockBackend *blk,
+                          QEMUSGList *sg, uint64_t offset, uint32_t align,
+                          void (*cb)(void *opaque, int ret), void *opaque)
+{
+    return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+                      dma_blk_barrier_write_io_func, blk, cb, opaque,
+                      DMA_DIRECTION_TO_DEVICE);
+}
+
+
+/* Jieun add */
+BlockAIOCB *dma_blk_fua_io_func(int64_t offset, QEMUIOVector *iov,
+                                  BlockCompletionFunc *cb, void *cb_opaque,
+                                  void *opaque)
+{
+    BlockBackend *blk = opaque;
+    return blk_aio_pfuav(blk, offset, iov, 0, cb, cb_opaque);
+}
+
+/* Jieun add */
+BlockAIOCB *dma_blk_fua(BlockBackend *blk,
+                          QEMUSGList *sg, uint64_t offset, uint32_t align,
+                          void (*cb)(void *opaque, int ret), void *opaque)
+{
+    return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+                      dma_blk_fua_io_func, blk, cb, opaque,
+                      DMA_DIRECTION_TO_DEVICE);
+}
+
 
 static uint64_t dma_buf_rw(uint8_t *ptr, int32_t len, QEMUSGList *sg,
                            DMADirection dir)
diff --git hw/block/dataplane/virtio-blk.c hw/block/dataplane/virtio-blk.c
index 101f32c..3e3438c 100644
--- hw/block/dataplane/virtio-blk.c
+++ hw/block/dataplane/virtio-blk.c
@@ -65,7 +65,8 @@ static void notify_guest_bh(void *opaque)
 
     memcpy(bitmap, s->batch_notify_vqs, sizeof(bitmap));
     memset(s->batch_notify_vqs, 0, sizeof(bitmap));
-
+	
+	//printf("[jieun-debug] number of vqs %d\n", nvqs);
     for (j = 0; j < nvqs; j += BITS_PER_LONG) {
         unsigned long bits = bitmap[j];
 
diff --git hw/block/nvme.c hw/block/nvme.c
index 85d2406..6171611 100644
--- hw/block/nvme.c
+++ hw/block/nvme.c
@@ -37,6 +37,9 @@
 #include "qemu/log.h"
 #include "trace.h"
 #include "nvme.h"
+/* Jieun add */
+//#define DEBUG
+#define BARRIER_FS
 
 #define NVME_GUEST_ERR(trace, fmt, ...) \
     do { \
@@ -362,26 +365,90 @@ static uint16_t nvme_rw(NvmeCtrl *n, NvmeNamespace *ns, NvmeCmd *cmd,
     if (nvme_map_prp(&req->qsg, &req->iov, prp1, prp2, data_size, n)) {
         block_acct_invalid(blk_get_stats(n->conf.blk), acct);
         return NVME_INVALID_FIELD | NVME_DNR;
-    }
+		}
 
-    dma_acct_start(n->conf.blk, &req->acct, &req->qsg, acct);
-    if (req->qsg.nsg > 0) {
-        req->has_sg = true;
-        req->aiocb = is_write ?
-            dma_blk_write(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
-                          nvme_rw_cb, req) :
-            dma_blk_read(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
-                         nvme_rw_cb, req);
-    } else {
-        req->has_sg = false;
-        req->aiocb = is_write ?
-            blk_aio_pwritev(n->conf.blk, data_offset, &req->iov, 0, nvme_rw_cb,
-                            req) :
-            blk_aio_preadv(n->conf.blk, data_offset, &req->iov, 0, nvme_rw_cb,
-                           req);
-    }
+		dma_acct_start(n->conf.blk, &req->acct, &req->qsg, acct);
 
-    return NVME_NO_COMPLETE;
+		if (req->qsg.nsg > 0) {
+
+						req->has_sg = true;
+
+
+		//	fprintf(stderr, "[jieun-debug]: QSG.NSG > 0\n");		
+
+						if (is_write) {
+										if (rw->control & NVME_RW_FUA) {
+
+														#ifdef DEBUG
+														fprintf(stderr, "[jieun-debug]: NVME_CMD_FUA\n");	
+														#endif
+														req->aiocb = dma_blk_fua(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
+																		nvme_rw_cb, req);
+										}
+										
+										#ifdef BARRIER_FS
+										else if (rw->control & NVME_RW_BARRIER) {
+											#ifdef DEBUG
+											fprintf(stderr, "[jieun-debug]: Barrier Write\n");		
+											#endif
+											req->aiocb = dma_blk_barrier_write(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
+																							nvme_rw_cb, req);
+
+										#endif
+										}
+										else {
+											#ifdef DEBUG
+												fprintf(stderr, "[jieun-debug]: NVME_CMD_WRITE\n");		
+#endif
+												req->aiocb = dma_blk_write(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
+																				nvme_rw_cb, req);
+
+										}
+
+
+									}
+		
+						else {	// Read
+
+										req->aiocb = dma_blk_read(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
+																		nvme_rw_cb, req);
+						}
+						/*
+
+						req->aiocb = is_write ?
+										dma_blk_write(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
+																		nvme_rw_cb, req) :
+										dma_blk_read(n->conf.blk, &req->qsg, data_offset, BDRV_SECTOR_SIZE,
+																		nvme_rw_cb, req);
+																		*/
+		} else {
+						req->has_sg = false;
+
+#ifdef DEBUG
+
+						fprintf(stderr, "[jieun-debug]: QSG.NSG < 0\n");		
+						if (is_write) {
+										if (rw->control & NVME_RW_FUA) {
+														fprintf(stderr, "[jieun-debug]: NVME_CMD_FUA\n");		
+										}
+
+										else {
+														fprintf(stderr, "[jieun-debug]: NVME_CMD_WRITE\n");		
+										}
+						}
+
+#endif
+
+
+						req->aiocb = is_write ?
+										blk_aio_pwritev(n->conf.blk, data_offset, &req->iov, 0, nvme_rw_cb,
+																		req) :
+										blk_aio_preadv(n->conf.blk, data_offset, &req->iov, 0, nvme_rw_cb,
+																		req);
+		}
+
+
+		return NVME_NO_COMPLETE;
 }
 
 static uint16_t nvme_io_cmd(NvmeCtrl *n, NvmeCmd *cmd, NvmeRequest *req)
@@ -397,8 +464,14 @@ static uint16_t nvme_io_cmd(NvmeCtrl *n, NvmeCmd *cmd, NvmeRequest *req)
     ns = &n->namespaces[nsid - 1];
     switch (cmd->opcode) {
     case NVME_CMD_FLUSH:
+#ifdef DEBUG
+		fprintf(stderr, "[jieun-debug]: NVME_CMD_FLUSH\n");
+#endif
         return nvme_flush(n, ns, cmd, req);
     case NVME_CMD_WRITE_ZEROS:
+#ifdef DEBUG
+		fprintf(stderr, "[jieun-debug]: NVME_CMD_WRITE_ZEROS\n");		
+#endif
         return nvme_write_zeros(n, ns, cmd, req);
     case NVME_CMD_WRITE:
     case NVME_CMD_READ:
@@ -519,6 +592,9 @@ static uint16_t nvme_create_sq(NvmeCtrl *n, NvmeCmd *cmd)
         trace_nvme_err_invalid_create_sq_qflags(NVME_SQ_FLAGS_PC(qflags));
         return NVME_INVALID_FIELD | NVME_DNR;
     }
+
+	// jieun debug
+	//fprintf(stderr,"[jieun-debug] Create SQ %d\n", sqid);
     sq = g_malloc0(sizeof(*sq));
     nvme_init_sq(sq, n, prp1, sqid, cqid, qsize + 1);
     return NVME_SUCCESS;
diff --git include/block/block.h include/block/block.h
index 68a667a..03c3c94 100644
--- include/block/block.h
+++ include/block/block.h
@@ -361,7 +361,9 @@ int bdrv_inactivate_all(void);
 
 /* Ensure contents are flushed to disk.  */
 int bdrv_flush(BlockDriverState *bs);
+int bdrv_barrier_write(BlockDriverState *bs);	//Jieun add
 int coroutine_fn bdrv_co_flush(BlockDriverState *bs);
+int coroutine_fn bdrv_co_barrier_write(BlockDriverState *bs);	//Jieun add
 int bdrv_flush_all(void);
 void bdrv_close_all(void);
 void bdrv_drain(BlockDriverState *bs);
diff --git include/block/block_int.h include/block/block_int.h
index c4dd1d4..507887f 100644
--- include/block/block_int.h
+++ include/block/block_int.h
@@ -149,6 +149,8 @@ struct BlockDriver {
         BlockCompletionFunc *cb, void *opaque);
     BlockAIOCB *(*bdrv_aio_flush)(BlockDriverState *bs,
         BlockCompletionFunc *cb, void *opaque);
+    BlockAIOCB *(*bdrv_aio_barrier_write)(BlockDriverState *bs,
+        BlockCompletionFunc *cb, void *opaque);	//Jieun add
     BlockAIOCB *(*bdrv_aio_pdiscard)(BlockDriverState *bs,
         int64_t offset, int bytes,
         BlockCompletionFunc *cb, void *opaque);
@@ -195,7 +197,14 @@ struct BlockDriver {
     int coroutine_fn (*bdrv_co_pwritev)(BlockDriverState *bs,
         uint64_t offset, uint64_t bytes, QEMUIOVector *qiov, int flags);
 
-    /*
+    /* Jieun add */
+		int coroutine_fn (*bdrv_co_pfua)(BlockDriverState *bs,
+        uint64_t offset, uint64_t bytes, QEMUIOVector *qiov, int flags);
+
+    /* Jieun add */
+		int coroutine_fn (*bdrv_co_pbarrier_write)(BlockDriverState *bs,
+        uint64_t offset, uint64_t bytes, QEMUIOVector *qiov, int flags);
+		/*
      * Efficiently zero a region of the disk image.  Typically an image format
      * would use a compact metadata representation to implement this.  This
      * function pointer may be NULL or return -ENOSUP and .bdrv_co_writev()
@@ -239,6 +248,7 @@ struct BlockDriver {
      * synchronization of the flush finishing callback.
      */
     int coroutine_fn (*bdrv_co_flush)(BlockDriverState *bs);
+		int coroutine_fn (*bdrv_co_barrier_write)(BlockDriverState *bs);	//Jieun add
 
     /*
      * Flushes all data that was already written to the OS all the way down to
@@ -796,6 +806,14 @@ int coroutine_fn bdrv_co_preadv(BdrvChild *child,
 int coroutine_fn bdrv_co_pwritev(BdrvChild *child,
     int64_t offset, unsigned int bytes, QEMUIOVector *qiov,
     BdrvRequestFlags flags);
+/* Jieun add */
+int coroutine_fn bdrv_co_pfua(BdrvChild *child,
+    int64_t offset, unsigned int bytes, QEMUIOVector *qiov,
+    BdrvRequestFlags flags);
+/* Jieun add */
+int coroutine_fn bdrv_co_pbarrier_write(BdrvChild *child,
+    int64_t offset, unsigned int bytes, QEMUIOVector *qiov,
+    BdrvRequestFlags flags);
 
 void bdrv_apply_subtree_drain(BdrvChild *child, BlockDriverState *new_parent);
 void bdrv_unapply_subtree_drain(BdrvChild *child, BlockDriverState *old_parent);
diff --git include/block/nvme.h include/block/nvme.h
index 849a6f3..5579b91 100644
--- include/block/nvme.h
+++ include/block/nvme.h
@@ -335,6 +335,7 @@ typedef struct NvmeRwCmd {
 enum {
     NVME_RW_LR                  = 1 << 15,
     NVME_RW_FUA                 = 1 << 14,
+    NVME_RW_BARRIER             = 1 << 9,	//Jieun add
     NVME_RW_DSM_FREQ_UNSPEC     = 0,
     NVME_RW_DSM_FREQ_TYPICAL    = 1,
     NVME_RW_DSM_FREQ_RARE       = 2,
diff --git include/block/raw-aio.h include/block/raw-aio.h
index a4cdbbf..3599541 100644
--- include/block/raw-aio.h
+++ include/block/raw-aio.h
@@ -23,11 +23,12 @@
 #define QEMU_AIO_WRITE        0x0002
 #define QEMU_AIO_IOCTL        0x0004
 #define QEMU_AIO_FLUSH        0x0008
+#define QEMU_AIO_BARRIER_WRITE	0x0040	/* Jieun add */
 #define QEMU_AIO_DISCARD      0x0010
 #define QEMU_AIO_WRITE_ZEROES 0x0020
 #define QEMU_AIO_TYPE_MASK \
         (QEMU_AIO_READ|QEMU_AIO_WRITE|QEMU_AIO_IOCTL|QEMU_AIO_FLUSH| \
-         QEMU_AIO_DISCARD|QEMU_AIO_WRITE_ZEROES)
+         QEMU_AIO_DISCARD|QEMU_AIO_WRITE_ZEROES|QEMU_AIO_BARRIER_WRITE)
 
 /* AIO flags */
 #define QEMU_AIO_MISALIGNED   0x1000
diff --git include/qemu/cutils.h include/qemu/cutils.h
index a663340..284af4e 100644
--- include/qemu/cutils.h
+++ include/qemu/cutils.h
@@ -124,6 +124,7 @@ int qemu_strnlen(const char *s, int max_len);
 char *qemu_strsep(char **input, const char *delim);
 time_t mktimegm(struct tm *tm);
 int qemu_fdatasync(int fd);
+int qemu_fdatabarrier(int fd); //Jieun add
 int fcntl_setfl(int fd, int flag);
 int qemu_parse_fd(const char *param);
 int qemu_strtoi(const char *nptr, const char **endptr, int base,
diff --git include/sysemu/block-backend.h include/sysemu/block-backend.h
index 92ab624..bc472bf 100644
--- include/sysemu/block-backend.h
+++ include/sysemu/block-backend.h
@@ -125,8 +125,20 @@ int coroutine_fn blk_co_preadv(BlockBackend *blk, int64_t offset,
 int coroutine_fn blk_co_pwritev(BlockBackend *blk, int64_t offset,
                                unsigned int bytes, QEMUIOVector *qiov,
                                BdrvRequestFlags flags);
+
 int blk_pwrite_zeroes(BlockBackend *blk, int64_t offset,
                       int bytes, BdrvRequestFlags flags);
+/* Jieun add */
+int coroutine_fn blk_co_pbarrier_write(BlockBackend *blk, int64_t offset,
+                                unsigned int bytes, QEMUIOVector *qiov,
+                                BdrvRequestFlags flags);
+/* Jieun add */
+int coroutine_fn blk_co_pfua(BlockBackend *blk, int64_t offset,
+                                unsigned int bytes, QEMUIOVector *qiov,
+                                BdrvRequestFlags flags);
+void blk_aio_fua_entry(void *opaque);
+void blk_aio_barrier_write_entry(void *opaque);
+
 BlockAIOCB *blk_aio_pwrite_zeroes(BlockBackend *blk, int64_t offset,
                                   int bytes, BdrvRequestFlags flags,
                                   BlockCompletionFunc *cb, void *opaque);
@@ -143,8 +155,18 @@ BlockAIOCB *blk_aio_preadv(BlockBackend *blk, int64_t offset,
 BlockAIOCB *blk_aio_pwritev(BlockBackend *blk, int64_t offset,
                             QEMUIOVector *qiov, BdrvRequestFlags flags,
                             BlockCompletionFunc *cb, void *opaque);
+
 BlockAIOCB *blk_aio_flush(BlockBackend *blk,
                           BlockCompletionFunc *cb, void *opaque);
+/* Jieun add */
+BlockAIOCB *blk_aio_pbarrierwritev(BlockBackend *blk, int64_t offset,
+                            QEMUIOVector *qiov, BdrvRequestFlags flags,
+                            BlockCompletionFunc *cb, void *opaque);
+/* Jieun add */
+BlockAIOCB *blk_aio_pfuav(BlockBackend *blk, int64_t offset,
+                            QEMUIOVector *qiov, BdrvRequestFlags flags,
+                            BlockCompletionFunc *cb, void *opaque);
+
 BlockAIOCB *blk_aio_pdiscard(BlockBackend *blk, int64_t offset, int bytes,
                              BlockCompletionFunc *cb, void *opaque);
 void blk_aio_cancel(BlockAIOCB *acb);
diff --git include/sysemu/dma.h include/sysemu/dma.h
index c228c66..81f7026 100644
--- include/sysemu/dma.h
+++ include/sysemu/dma.h
@@ -208,6 +208,17 @@ BlockAIOCB *dma_blk_read(BlockBackend *blk,
 BlockAIOCB *dma_blk_write(BlockBackend *blk,
                           QEMUSGList *sg, uint64_t offset, uint32_t align,
                           BlockCompletionFunc *cb, void *opaque);
+
+/* Jieun add */
+BlockAIOCB *dma_blk_barrier_write(BlockBackend *blk,
+                          QEMUSGList *sg, uint64_t offset, uint32_t align,
+                          void (*cb)(void *opaque, int ret), void *opaque);
+
+/* Jieun add */
+BlockAIOCB *dma_blk_fua(BlockBackend *blk,
+                          QEMUSGList *sg, uint64_t offset, uint32_t align,
+                          void (*cb)(void *opaque, int ret), void *opaque);
+
 uint64_t dma_buf_read(uint8_t *ptr, int32_t len, QEMUSGList *sg);
 uint64_t dma_buf_write(uint8_t *ptr, int32_t len, QEMUSGList *sg);
 
diff --git util/cutils.c util/cutils.c
index 0de69e6..1cb4738 100644
--- util/cutils.c
+++ util/cutils.c
@@ -32,6 +32,8 @@
 #include "qemu/cutils.h"
 #include "qemu/error-report.h"
 
+/* Jieun add */
+#define DEBUG
 void strpadcpy(char *buf, int buf_size, const char *str, char pad)
 {
     int len = qemu_strnlen(str, buf_size);
@@ -146,6 +148,11 @@ time_t mktimegm(struct tm *tm)
     return t;
 }
 
+/* Jieun add */
+int qemu_fdatabarrier(int fd)
+{
+		return syscall(315,fd);
+}
 /*
  * Make sure data goes on disk, but if possible do not bother to
  * write out the inode just for timestamp updates.
@@ -155,6 +162,7 @@ time_t mktimegm(struct tm *tm)
  */
 int qemu_fdatasync(int fd)
 {
+
 #ifdef CONFIG_FDATASYNC
     return fdatasync(fd);
 #else
